{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Embeddings Exploration\n",
    "\n",
    "Embeddings are how LLMs understand **meaning**. While tokenization breaks text into pieces, embeddings capture what those pieces *mean* by representing them as vectors in a high-dimensional space.\n",
    "\n",
    "The key insight: **similar meanings = nearby vectors**. \"Dog\" and \"puppy\" end up close together, while \"dog\" and \"refrigerator\" end up far apart.\n",
    "\n",
    "This notebook lets you experiment hands-on with embeddings using `sentence-transformers`, a free library that runs entirely on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install and import the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sentence-transformers if you don't have it\n",
    "!pip install sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a small, fast model that produces good embeddings\n",
    "# This will download the model on first run (~90MB)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Model loaded!\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Getting Embeddings\n",
    "\n",
    "Let's see what an embedding actually looks like - it's just a list of numbers (a vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 'hello'\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "Embedding shape: (384,)\n",
      "\n",
      "First 10 values: [-0.06277174  0.05495885  0.05216482  0.08579    -0.08274895 -0.07457299\n",
      "  0.06855471  0.01839639 -0.08201139 -0.03738479]\n",
      "\n",
      "Min value: -0.1444\n",
      "Max value: 0.2981\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding for a word\n",
    "word = \"hello\"\n",
    "embedding = model.encode(word)\n",
    "\n",
    "print(f\"Word: '{word}'\")\n",
    "print(f\"Embedding type: {type(embedding)}\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"\\nFirst 10 values: {embedding[:10]}\")\n",
    "print(f\"\\nMin value: {embedding.min():.4f}\")\n",
    "print(f\"Max value: {embedding.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This word is represented by 384 numbers.\n",
      "\n",
      "Think of it as a point in 384-dimensional space.\n",
      "Words with similar meanings are points that are close together.\n"
     ]
    }
   ],
   "source": [
    "# The embedding is a 384-dimensional vector\n",
    "# Each dimension captures some aspect of the meaning\n",
    "print(f\"This word is represented by {len(embedding)} numbers.\")\n",
    "print(f\"\\nThink of it as a point in 384-dimensional space.\")\n",
    "print(f\"Words with similar meanings are points that are close together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Cosine Similarity\n",
    "\n",
    "To measure how similar two embeddings are, we use **cosine similarity**. It measures the angle between two vectors:\n",
    "\n",
    "- **1.0** = identical direction (very similar meaning)\n",
    "- **0.0** = perpendicular (unrelated)\n",
    "- **-1.0** = opposite direction (opposite meaning)\n",
    "\n",
    "In practice, most text embeddings fall between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple 3D vectors:\n",
      "  Same direction: 1.00\n",
      "  Perpendicular:  0.00\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Formula: cos(theta) = (A . B) / (||A|| * ||B||)\n",
    "    \n",
    "    Where:\n",
    "    - A . B is the dot product\n",
    "    - ||A|| and ||B|| are the magnitudes (lengths) of the vectors\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "# Test it with a simple example\n",
    "vec_a = np.array([1, 0, 0])\n",
    "vec_b = np.array([1, 0, 0])  # Same direction\n",
    "vec_c = np.array([0, 1, 0])  # Perpendicular\n",
    "\n",
    "print(\"Simple 3D vectors:\")\n",
    "print(f\"  Same direction: {cosine_similarity(vec_a, vec_b):.2f}\")\n",
    "print(f\"  Perpendicular:  {cosine_similarity(vec_a, vec_c):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Semantic Similarity in Action\n",
    "\n",
    "Now for the magic - let's see how well the model captures semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_words(word1, word2):\n",
    "    \"\"\"Get embeddings for two words and compute their similarity.\"\"\"\n",
    "    emb1 = model.encode(word1)\n",
    "    emb2 = model.encode(word2)\n",
    "    similarity = cosine_similarity(emb1, emb2)\n",
    "    return similarity\n",
    "\n",
    "def show_similarity(word1, word2, expected=\"\"):\n",
    "    \"\"\"Display the similarity between two words.\"\"\"\n",
    "    sim = compare_words(word1, word2)\n",
    "    bar = \"#\" * int(sim * 30)  # Visual bar\n",
    "    note = f\" ({expected})\" if expected else \"\"\n",
    "    print(f\"'{word1}' vs '{word2}': {sim:.3f} {bar}{note}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same concept, different words:\n",
      "'dog' vs 'puppy': 0.804 ######################## (high - same animal)\n",
      "'car' vs 'automobile': 0.865 ######################### (high - synonyms)\n",
      "'happy' vs 'joyful': 0.684 #################### (high - synonyms)\n",
      "'big' vs 'large': 0.807 ######################## (high - synonyms)\n"
     ]
    }
   ],
   "source": [
    "# Expected: high similarity - same concept\n",
    "print(\"Same concept, different words:\")\n",
    "show_similarity(\"dog\", \"puppy\", \"high - same animal\")\n",
    "show_similarity(\"car\", \"automobile\", \"high - synonyms\")\n",
    "show_similarity(\"happy\", \"joyful\", \"high - synonyms\")\n",
    "show_similarity(\"big\", \"large\", \"high - synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Related concepts:\n",
      "'dog' vs 'cat': 0.661 ################### (medium - both pets)\n",
      "'coffee' vs 'tea': 0.616 ################## (medium - both beverages)\n",
      "'doctor' vs 'nurse': 0.608 ################## (medium - both medical)\n",
      "'king' vs 'queen': 0.681 #################### (medium - both royalty)\n"
     ]
    }
   ],
   "source": [
    "# Expected: medium similarity - related but different\n",
    "print(\"\\nRelated concepts:\")\n",
    "show_similarity(\"dog\", \"cat\", \"medium - both pets\")\n",
    "show_similarity(\"coffee\", \"tea\", \"medium - both beverages\")\n",
    "show_similarity(\"doctor\", \"nurse\", \"medium - both medical\")\n",
    "show_similarity(\"king\", \"queen\", \"medium - both royalty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unrelated concepts:\n",
      "'dog' vs 'refrigerator': 0.246 ####### (low - unrelated)\n",
      "'banana' vs 'democracy': 0.186 ##### (low - unrelated)\n",
      "'laptop' vs 'elephant': 0.370 ########### (low - unrelated)\n",
      "'music' vs 'mathematics': 0.381 ########### (low-medium?)\n"
     ]
    }
   ],
   "source": [
    "# Expected: low similarity - unrelated\n",
    "print(\"\\nUnrelated concepts:\")\n",
    "show_similarity(\"dog\", \"refrigerator\", \"low - unrelated\")\n",
    "show_similarity(\"banana\", \"democracy\", \"low - unrelated\")\n",
    "show_similarity(\"laptop\", \"elephant\", \"low - unrelated\")\n",
    "show_similarity(\"music\", \"mathematics\", \"low-medium?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interesting cases (opposites):\n",
      "'hot' vs 'cold': 0.519 ############### (often high! - same concept, opposite ends)\n",
      "'love' vs 'hate': 0.488 ############## (often high! - same concept, opposite ends)\n",
      "'up' vs 'down': 0.673 #################### (often high! - same concept, opposite ends)\n",
      "\n",
      "(Note: Opposites often have HIGH similarity because they relate to the same concept!\n",
      "This is a limitation of simple embedding similarity.)\n"
     ]
    }
   ],
   "source": [
    "# Interesting cases - opposites and antonyms\n",
    "print(\"\\nInteresting cases (opposites):\")\n",
    "show_similarity(\"hot\", \"cold\", \"often high! - same concept, opposite ends\")\n",
    "show_similarity(\"love\", \"hate\", \"often high! - same concept, opposite ends\")\n",
    "show_similarity(\"up\", \"down\", \"often high! - same concept, opposite ends\")\n",
    "\n",
    "print(\"\\n(Note: Opposites often have HIGH similarity because they relate to the same concept!\")\n",
    "print(\"This is a limitation of simple embedding similarity.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "**Try it yourself:** What other word pairs would you like to compare? Add some below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'python' vs 'snake': 0.444 #############\n",
      "'python' vs 'programming': 0.613 ##################\n",
      "'apple' vs 'fruit': 0.537 ################\n",
      "'apple' vs 'microsoft': 0.493 ##############\n"
     ]
    }
   ],
   "source": [
    "# Your turn - try your own word pairs\n",
    "show_similarity(\"python\", \"snake\")\n",
    "show_similarity(\"python\", \"programming\")\n",
    "show_similarity(\"apple\", \"fruit\")\n",
    "show_similarity(\"apple\", \"microsoft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 5. Sentences vs Words\n",
    "\n",
    "The real power of modern embedding models is that they work on **whole sentences**, not just words. The meaning of a sentence is more than the sum of its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'bank' has multiple meanings.\n",
      "\n",
      "Sentence 1 (financial) vs Sentence 2 (river): 0.312\n",
      "\n",
      "The sentences are somewhat different because the context differs!\n"
     ]
    }
   ],
   "source": [
    "# Single word embedding\n",
    "word_emb = model.encode(\"bank\")\n",
    "\n",
    "# Sentence embeddings - same word, different meanings\n",
    "sent1_emb = model.encode(\"I need to deposit money at the bank\")\n",
    "sent2_emb = model.encode(\"We had a picnic by the river bank\")\n",
    "\n",
    "print(\"The word 'bank' has multiple meanings.\")\n",
    "print(f\"\\nSentence 1 (financial) vs Sentence 2 (river): {cosine_similarity(sent1_emb, sent2_emb):.3f}\")\n",
    "print(\"\\nThe sentences are somewhat different because the context differs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence similarities:\n",
      "\n",
      "0.577: 'The cat sat on the mat...' vs 'A feline was resting on the rug...'\n",
      "0.072: 'The cat sat on the mat...' vs 'The dog ran in the park...'\n",
      "0.020: 'The cat sat on the mat...' vs 'I love programming in Python...'\n",
      "0.118: 'A feline was resting on the rug...' vs 'The dog ran in the park...'\n",
      "0.040: 'A feline was resting on the rug...' vs 'I love programming in Python...'\n",
      "0.055: 'The dog ran in the park...' vs 'I love programming in Python...'\n"
     ]
    }
   ],
   "source": [
    "# Same meaning, different words\n",
    "sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"A feline was resting on the rug\",\n",
    "    \"The dog ran in the park\",\n",
    "    \"I love programming in Python\",\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(\"Sentence similarities:\\n\")\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i + 1, len(sentences)):\n",
    "        sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        print(f\"{sim:.3f}: '{sentences[i][:40]}...' vs '{sentences[j][:40]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase detection (same meaning, different words):\n",
      "\n",
      "0.761: 'How old are you?' <-> 'What is your age?'\n",
      "0.805: 'The movie was great' <-> 'I really enjoyed the film'\n",
      "0.791: 'It's raining outside' <-> 'The weather is wet'\n"
     ]
    }
   ],
   "source": [
    "# Paraphrases - same meaning, completely different words\n",
    "paraphrase_pairs = [\n",
    "    (\"How old are you?\", \"What is your age?\"),\n",
    "    (\"The movie was great\", \"I really enjoyed the film\"),\n",
    "    (\"It's raining outside\", \"The weather is wet\"),\n",
    "]\n",
    "\n",
    "print(\"Paraphrase detection (same meaning, different words):\\n\")\n",
    "for sent1, sent2 in paraphrase_pairs:\n",
    "    sim = compare_words(sent1, sent2)\n",
    "    print(f\"{sim:.3f}: '{sent1}' <-> '{sent2}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 6. Practical Example: Semantic Search\n",
    "\n",
    "One of the most useful applications of embeddings is **semantic search** - finding content by meaning, not just keywords.\n",
    "\n",
    "Instead of exact string matching, we find documents whose embeddings are closest to the query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 10 documents\n",
      "Each document is represented by a 384-dimensional vector\n"
     ]
    }
   ],
   "source": [
    "# Our \"document\" collection\n",
    "documents = [\n",
    "    \"Python is a popular programming language for data science\",\n",
    "    \"Machine learning models can predict future outcomes\",\n",
    "    \"The weather forecast shows rain tomorrow\",\n",
    "    \"Neural networks are inspired by the human brain\",\n",
    "    \"I made a delicious pasta for dinner last night\",\n",
    "    \"Deep learning requires large amounts of training data\",\n",
    "    \"The stock market showed gains today\",\n",
    "    \"Transformers have revolutionized natural language processing\",\n",
    "    \"My cat loves to sleep in sunny spots\",\n",
    "    \"GPT models generate human-like text\",\n",
    "]\n",
    "\n",
    "# Pre-compute embeddings for all documents (in practice, you'd store these)\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "print(f\"Indexed {len(documents)} documents\")\n",
    "print(f\"Each document is represented by a {doc_embeddings.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Find the most similar documents to a query.\n",
    "    \n",
    "    This is the basic algorithm behind semantic search:\n",
    "    1. Embed the query\n",
    "    2. Compare to all document embeddings\n",
    "    3. Return the closest matches\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query)\n",
    "    \n",
    "    # Calculate similarity to each document\n",
    "    similarities = []\n",
    "    for i, doc_emb in enumerate(doc_embeddings):\n",
    "        sim = cosine_similarity(query_embedding, doc_emb)\n",
    "        similarities.append((sim, i))\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    similarities.sort(reverse=True)\n",
    "    \n",
    "    # Return top results\n",
    "    print(f\"Query: '{query}'\\n\")\n",
    "    print(\"Top results:\")\n",
    "    for rank, (sim, idx) in enumerate(similarities[:top_k], 1):\n",
    "        print(f\"  {rank}. [{sim:.3f}] {documents[idx]}\")\n",
    "    \n",
    "    return similarities[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'AI and artificial intelligence'\n",
      "\n",
      "Top results:\n",
      "  1. [0.454] Neural networks are inspired by the human brain\n",
      "  2. [0.321] Machine learning models can predict future outcomes\n",
      "  3. [0.215] Transformers have revolutionized natural language processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.float32(0.45375124), 3),\n",
       " (np.float32(0.32095325), 1),\n",
       " (np.float32(0.21459457), 7)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try some searches!\n",
    "semantic_search(\"AI and artificial intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'coding'\n",
      "\n",
      "Top results:\n",
      "  1. [0.287] Python is a popular programming language for data science\n",
      "  2. [0.277] GPT models generate human-like text\n",
      "  3. [0.267] Neural networks are inspired by the human brain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.float32(0.28735948), 0),\n",
       " (np.float32(0.27661642), 9),\n",
       " (np.float32(0.26702595), 3)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice: we don't need exact keyword matches\n",
    "semantic_search(\"coding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How do large language models work?'\n",
      "\n",
      "Top results:\n",
      "  1. [0.403] Transformers have revolutionized natural language processing\n",
      "  2. [0.393] GPT models generate human-like text\n",
      "  3. [0.294] Deep learning requires large amounts of training data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.float32(0.40281165), 7),\n",
       " (np.float32(0.39343098), 9),\n",
       " (np.float32(0.2939294), 5)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Works with questions too\n",
    "semantic_search(\"How do large language models work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'pets and animals'\n",
      "\n",
      "Top results:\n",
      "  1. [0.243] My cat loves to sleep in sunny spots\n",
      "  2. [0.171] Neural networks are inspired by the human brain\n",
      "  3. [0.123] Python is a popular programming language for data science\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.float32(0.2425824), 8),\n",
       " (np.float32(0.17082085), 3),\n",
       " (np.float32(0.122809336), 0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Completely different domain\n",
    "semantic_search(\"pets and animals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'cooking and food'\n",
      "\n",
      "Top results:\n",
      "  1. [0.367] I made a delicious pasta for dinner last night\n",
      "  2. [0.136] My cat loves to sleep in sunny spots\n",
      "  3. [0.097] Machine learning models can predict future outcomes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.float32(0.3670238), 4),\n",
       " (np.float32(0.13638993), 8),\n",
       " (np.float32(0.0970244), 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Food-related query\n",
    "semantic_search(\"cooking and food\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "**Key insight:** Semantic search finds relevant content even when the exact words don't match. This is why it's so powerful for:\n",
    "\n",
    "- Search engines\n",
    "- Recommendation systems\n",
    "- RAG (Retrieval Augmented Generation) for LLMs\n",
    "- Finding similar documents\n",
    "- Clustering content by topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. **Embeddings are vectors** - Text gets converted to lists of numbers (384 dimensions in our model)\n",
    "\n",
    "2. **Meaning = Position** - Similar meanings end up as nearby points in the vector space\n",
    "\n",
    "3. **Cosine similarity** - Measures how similar two embeddings are (0-1 for text, where 1 = identical)\n",
    "\n",
    "4. **Sentences, not just words** - Modern models capture full sentence meaning, including context\n",
    "\n",
    "5. **Semantic search** - Find relevant content by meaning, not just keyword matching\n",
    "\n",
    "6. **Limitations** - Antonyms often have high similarity (they're related concepts). Embeddings don't capture logic or reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "Use these cells to experiment with your own text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell-32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Your first text here' vs 'Your second text here': 0.843 #########################\n"
     ]
    }
   ],
   "source": [
    "# Compare any two pieces of text\n",
    "text1 = \"Your first text here\"\n",
    "text2 = \"Your second text here\"\n",
    "\n",
    "show_similarity(text1, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cell-33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'your search query'\n",
      "\n",
      "  [0.228] Add your own documents here\n",
      "  [0.061] Make them about topics you care about\n",
      "  [0.510] Then search for related content\n"
     ]
    }
   ],
   "source": [
    "# Add your own documents and search them\n",
    "my_documents = [\n",
    "    \"Add your own documents here\",\n",
    "    \"Make them about topics you care about\",\n",
    "    \"Then search for related content\",\n",
    "]\n",
    "\n",
    "# Update the embeddings\n",
    "my_doc_embeddings = model.encode(my_documents)\n",
    "\n",
    "# Now search\n",
    "my_query = \"your search query\"\n",
    "query_emb = model.encode(my_query)\n",
    "\n",
    "print(f\"Query: '{my_query}'\\n\")\n",
    "for i, doc in enumerate(my_documents):\n",
    "    sim = cosine_similarity(query_emb, my_doc_embeddings[i])\n",
    "    print(f\"  [{sim:.3f}] {doc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
