{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Exploration\n",
    "\n",
    "Tokenization is how LLMs break text into pieces they can understand. Instead of reading character-by-character or word-by-word, they use **tokens** - chunks that can be whole words, parts of words, or even single characters.\n",
    "\n",
    "This notebook lets you experiment hands-on with tokenization using `tiktoken`, the tokenizer OpenAI uses for GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install and import the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tiktoken if you don't have it\n",
    "!pip install tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded!\n",
      "Vocabulary size: 100,277 tokens\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load the encoding used by GPT-4 and GPT-3.5-turbo\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"Tokenizer loaded!\")\n",
    "print(f\"Vocabulary size: {enc.n_vocab:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Tokenization\n",
    "\n",
    "Let's see how text gets converted to tokens and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Hello world '\n",
      "Token IDs: [9906, 1917, 220]\n",
      "Number of tokens: 3\n"
     ]
    }
   ],
   "source": [
    "# Encode text to tokens\n",
    "text = \"Hello world \"\n",
    "tokens = enc.encode(text)\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded: 'Hello world '\n",
      "Round-trip successful: True\n"
     ]
    }
   ],
   "source": [
    "# Decode tokens back to text\n",
    "decoded = enc.decode(tokens)\n",
    "print(f\"Decoded: '{decoded}'\")\n",
    "print(f\"Round-trip successful: {text == decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Hello world'\n",
      "Total tokens: 2\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   9906 -> 'Hello'\n",
      "  2. ID   1917 -> ' world'\n"
     ]
    }
   ],
   "source": [
    "# See what each token represents\n",
    "def show_tokens(text):\n",
    "    \"\"\"Display each token ID and what text it represents.\"\"\"\n",
    "    tokens = enc.encode(text)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Total tokens: {len(tokens)}\")\n",
    "    print(\"\\nBreakdown:\")\n",
    "    for i, token_id in enumerate(tokens):\n",
    "        token_text = enc.decode([token_id])\n",
    "        print(f\"  {i+1}. ID {token_id:>6} -> '{token_text}'\")\n",
    "\n",
    "show_tokens(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it yourself:** Change the text above to see how different phrases get tokenized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Case Sensitivity\n",
    "\n",
    "Tokenizers treat uppercase and lowercase differently. The same word in different cases becomes different tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ChatGPT' -> 3 tokens: [16047, 38, 2898]\n",
      "'chatgpt' -> 3 tokens: [9884, 70, 418]\n",
      "'CHATGPT' -> 3 tokens: [61817, 38, 2898]\n",
      "'Chatgpt' -> 3 tokens: [16047, 70, 418]\n"
     ]
    }
   ],
   "source": [
    "# Compare different cases of the same word\n",
    "words = [\"ChatGPT\", \"chatgpt\", \"CHATGPT\", \"Chatgpt\"]\n",
    "\n",
    "for word in words:\n",
    "    tokens = enc.encode(word)\n",
    "    print(f\"'{word}' -> {len(tokens)} tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Text: 'ChatGPT'\n",
      "Total tokens: 3\n",
      "\n",
      "Breakdown:\n",
      "  1. ID  16047 -> 'Chat'\n",
      "  2. ID     38 -> 'G'\n",
      "  3. ID   2898 -> 'PT'\n",
      "==================================================\n",
      "Text: 'chatgpt'\n",
      "Total tokens: 3\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   9884 -> 'chat'\n",
      "  2. ID     70 -> 'g'\n",
      "  3. ID    418 -> 'pt'\n",
      "==================================================\n",
      "Text: 'CHATGPT'\n",
      "Total tokens: 3\n",
      "\n",
      "Breakdown:\n",
      "  1. ID  61817 -> 'CHAT'\n",
      "  2. ID     38 -> 'G'\n",
      "  3. ID   2898 -> 'PT'\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Look at each version in detail\n",
    "print(\"=\" * 50)\n",
    "for word in [\"ChatGPT\", \"chatgpt\", \"CHATGPT\"]:\n",
    "    show_tokens(word)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** \"ChatGPT\" (mixed case) might tokenize differently than \"chatgpt\" (lowercase). This means:\n",
    "- Different token counts\n",
    "- Different costs\n",
    "- Potentially different model behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Splitting\n",
    "\n",
    "Common words often get a single token. Rare or complex words get split into pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common words:\n",
      "  'the' -> 1 token(s)\n",
      "  'hello' -> 1 token(s)\n",
      "  'world' -> 1 token(s)\n",
      "  'is' -> 1 token(s)\n",
      "  'a' -> 1 token(s)\n",
      "  'computer' -> 1 token(s)\n"
     ]
    }
   ],
   "source": [
    "# Common words - usually single tokens\n",
    "common_words = [\"the\", \"hello\", \"world\", \"is\", \"a\", \"computer\"]\n",
    "\n",
    "print(\"Common words:\")\n",
    "for word in common_words:\n",
    "    tokens = enc.encode(word)\n",
    "    print(f\"  '{word}' -> {len(tokens)} token(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex words:\n",
      "Text: 'tokenization'\n",
      "Total tokens: 2\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   5963 -> 'token'\n",
      "  2. ID   2065 -> 'ization'\n",
      "\n",
      "Text: 'unbelievable'\n",
      "Total tokens: 3\n",
      "\n",
      "Breakdown:\n",
      "  1. ID    359 -> 'un'\n",
      "  2. ID  32898 -> 'belie'\n",
      "  3. ID  24694 -> 'vable'\n",
      "\n",
      "Text: 'cryptocurrency'\n",
      "Total tokens: 2\n",
      "\n",
      "Breakdown:\n",
      "  1. ID  49225 -> 'crypt'\n",
      "  2. ID  78580 -> 'ocurrency'\n",
      "\n",
      "Text: 'anthropomorphic'\n",
      "Total tokens: 3\n",
      "\n",
      "Breakdown:\n",
      "  1. ID  32329 -> 'anth'\n",
      "  2. ID    897 -> 'rop'\n",
      "  3. ID  71017 -> 'omorphic'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Longer/rarer words - often split\n",
    "complex_words = [\"tokenization\", \"unbelievable\", \"cryptocurrency\", \"anthropomorphic\"]\n",
    "\n",
    "print(\"Complex words:\")\n",
    "for word in complex_words:\n",
    "    show_tokens(word)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made-up words:\n",
      "Text: 'flurbtastic'\n",
      "Total tokens: 4\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   1517 -> 'fl'\n",
      "  2. ID   9225 -> 'urb'\n",
      "  3. ID     83 -> 't'\n",
      "  4. ID   5174 -> 'astic'\n",
      "\n",
      "Text: 'snorkelwumpus'\n",
      "Total tokens: 6\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   9810 -> 'sn'\n",
      "  2. ID    672 -> 'ork'\n",
      "  3. ID    301 -> 'el'\n",
      "  4. ID     86 -> 'w'\n",
      "  5. ID   1538 -> 'ump'\n",
      "  6. ID    355 -> 'us'\n",
      "\n",
      "Text: 'blorpification'\n",
      "Total tokens: 3\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   2067 -> 'bl'\n",
      "  2. ID  31215 -> 'orp'\n",
      "  3. ID   2461 -> 'ification'\n",
      "\n",
      "Text: 'xyzzyplugh'\n",
      "Total tokens: 4\n",
      "\n",
      "Breakdown:\n",
      "  1. ID  29954 -> 'xyz'\n",
      "  2. ID   4341 -> 'zy'\n",
      "  3. ID    501 -> 'pl'\n",
      "  4. ID   7595 -> 'ugh'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Made-up words get split heavily\n",
    "made_up = [\"flurbtastic\", \"snorkelwumpus\", \"blorpification\", \"xyzzyplugh\"]\n",
    "\n",
    "print(\"Made-up words:\")\n",
    "for word in made_up:\n",
    "    show_tokens(word)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight:** The tokenizer learned from real text. Words it saw often become single tokens. Rare combinations get broken into smaller pieces it does recognize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spaces Matter\n",
    "\n",
    "Spaces aren't always separate tokens. They often attach to the word that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without leading space:\n",
      "Text: 'Hello'\n",
      "Total tokens: 1\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   9906 -> 'Hello'\n",
      "\n",
      "With leading space:\n",
      "Text: ' Hello'\n",
      "Total tokens: 1\n",
      "\n",
      "Breakdown:\n",
      "  1. ID  22691 -> ' Hello'\n"
     ]
    }
   ],
   "source": [
    "# Compare with and without leading space\n",
    "print(\"Without leading space:\")\n",
    "show_tokens(\"Hello\")\n",
    "print()\n",
    "print(\"With leading space:\")\n",
    "show_tokens(\" Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Hello world how are you'\n",
      "Total tokens: 5\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   9906 -> 'Hello'\n",
      "  2. ID   1917 -> ' world'\n",
      "  3. ID   1268 -> ' how'\n",
      "  4. ID    527 -> ' are'\n",
      "  5. ID    499 -> ' you'\n"
     ]
    }
   ],
   "source": [
    "# See how spaces work in sentences\n",
    "show_tokens(\"Hello world how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** In the sentence above, most words after the first have their space attached. The tokenizer represents \" world\" (space+word) as a single token, different from \"world\" (no space).\n",
    "\n",
    "This is why `\" Hello\"` and `\"Hello\"` produce different tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Hello  world'\n",
      "Total tokens: 3\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   9906 -> 'Hello'\n",
      "  2. ID    220 -> ' '\n",
      "  3. ID   1917 -> ' world'\n"
     ]
    }
   ],
   "source": [
    "# What about multiple spaces?\n",
    "show_tokens(\"Hello  world\")  # two spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Token Counting\n",
    "\n",
    "Let's build a simple function to count tokens, then compare different ways of saying the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello world' = 2 tokens\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text):\n",
    "    \"\"\"Count the number of tokens in a text string.\"\"\"\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "# Test it\n",
    "print(f\"'Hello world' = {count_tokens('Hello world')} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same request, different token counts:\n",
      "\n",
      " 5 tokens: 'Please summarize this document.'\n",
      " 5 tokens: 'Summarize this.'\n",
      " 3 tokens: 'TL;DR'\n",
      "16 tokens: 'Can you please provide a brief summary of the contents of this document for me?'\n"
     ]
    }
   ],
   "source": [
    "# Compare different phrasings of the same idea\n",
    "phrasings = [\n",
    "    \"Please summarize this document.\",\n",
    "    \"Summarize this.\",\n",
    "    \"TL;DR\",\n",
    "    \"Can you please provide a brief summary of the contents of this document for me?\",\n",
    "]\n",
    "\n",
    "print(\"Same request, different token counts:\\n\")\n",
    "for phrase in phrasings:\n",
    "    tokens = count_tokens(phrase)\n",
    "    print(f\"{tokens:2} tokens: '{phrase}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number tokenization:\n",
      "Text: '1'\n",
      "Total tokens: 1\n",
      "\n",
      "Breakdown:\n",
      "  1. ID     16 -> '1'\n",
      "\n",
      "Text: '12'\n",
      "Total tokens: 1\n",
      "\n",
      "Breakdown:\n",
      "  1. ID    717 -> '12'\n",
      "\n",
      "Text: '123'\n",
      "Total tokens: 1\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   4513 -> '123'\n",
      "\n",
      "Text: '1234'\n",
      "Total tokens: 2\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   4513 -> '123'\n",
      "  2. ID     19 -> '4'\n",
      "\n",
      "Text: '12345'\n",
      "Total tokens: 2\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   4513 -> '123'\n",
      "  2. ID   1774 -> '45'\n",
      "\n",
      "Text: '123456789'\n",
      "Total tokens: 3\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   4513 -> '123'\n",
      "  2. ID  10961 -> '456'\n",
      "  3. ID  16474 -> '789'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Numbers can be tricky\n",
    "numbers = [\"1\", \"12\", \"123\", \"1234\", \"12345\", \"123456789\"]\n",
    "\n",
    "print(\"Number tokenization:\")\n",
    "for num in numbers:\n",
    "    show_tokens(num)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment:** Try writing the same prompt two different ways and compare the token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: 13 tokens\n",
      "Prompt 2: 6 tokens\n",
      "Difference: 7 tokens\n"
     ]
    }
   ],
   "source": [
    "# Your turn: compare two prompts\n",
    "prompt1 = \"Explain quantum computing to me like I'm five years old.\"\n",
    "prompt2 = \"Explain quantum computing simply.\"\n",
    "\n",
    "print(f\"Prompt 1: {count_tokens(prompt1)} tokens\")\n",
    "print(f\"Prompt 2: {count_tokens(prompt2)} tokens\")\n",
    "print(f\"Difference: {count_tokens(prompt1) - count_tokens(prompt2)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Implications\n",
    "\n",
    "Understanding tokenization matters for two big reasons: **cost** and **context limits**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Costs\n",
    "\n",
    "OpenAI and other providers charge per token. Both input and output tokens cost money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbose prompt:\n",
      "  35 tokens, ~$0.0004 input cost\n",
      "\n",
      "Concise prompt:\n",
      "  7 tokens, ~$0.0001 input cost\n",
      "\n",
      "Savings per request: ~$0.0003\n",
      "At 1000 requests/day: ~$0.28/day\n"
     ]
    }
   ],
   "source": [
    "# Rough cost calculation (prices change - these are illustrative)\n",
    "# GPT-4 Turbo pricing as of early 2024: $0.01/1K input, $0.03/1K output\n",
    "\n",
    "def estimate_cost(text, input_price_per_1k=0.01, output_price_per_1k=0.03):\n",
    "    \"\"\"Estimate cost for a prompt (assuming similar length response).\"\"\"\n",
    "    tokens = count_tokens(text)\n",
    "    input_cost = (tokens / 1000) * input_price_per_1k\n",
    "    # Assume output is roughly same length (very rough estimate)\n",
    "    output_cost = (tokens / 1000) * output_price_per_1k\n",
    "    return tokens, input_cost, output_cost\n",
    "\n",
    "# Compare verbose vs concise\n",
    "verbose = \"\"\"I would greatly appreciate it if you could please provide me with \n",
    "a comprehensive and detailed explanation of how machine learning algorithms \n",
    "work, including all the mathematical foundations and practical applications.\"\"\"\n",
    "\n",
    "concise = \"Explain how machine learning works.\"\n",
    "\n",
    "v_tokens, v_in, v_out = estimate_cost(verbose)\n",
    "c_tokens, c_in, c_out = estimate_cost(concise)\n",
    "\n",
    "print(\"Verbose prompt:\")\n",
    "print(f\"  {v_tokens} tokens, ~${v_in:.4f} input cost\")\n",
    "print()\n",
    "print(\"Concise prompt:\")\n",
    "print(f\"  {c_tokens} tokens, ~${c_in:.4f} input cost\")\n",
    "print()\n",
    "print(f\"Savings per request: ~${v_in - c_in:.4f}\")\n",
    "print(f\"At 1000 requests/day: ~${(v_in - c_in) * 1000:.2f}/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Window Limits\n",
    "\n",
    "Every model has a maximum context window (total tokens for input + output combined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate capacity (leaving room for response):\n",
      "\n",
      "GPT-3.5 Turbo:\n",
      "  12,288 tokens for input (~21 pages of text)\n",
      "GPT-4 Turbo:\n",
      "  96,000 tokens for input (~165 pages of text)\n",
      "GPT-4o:\n",
      "  96,000 tokens for input (~165 pages of text)\n",
      "Claude 3 Sonnet:\n",
      "  150,000 tokens for input (~257 pages of text)\n",
      "Claude 3 Opus:\n",
      "  150,000 tokens for input (~257 pages of text)\n"
     ]
    }
   ],
   "source": [
    "# Common context window sizes\n",
    "context_windows = {\n",
    "    \"GPT-3.5 Turbo\": 16_385,\n",
    "    \"GPT-4 Turbo\": 128_000,\n",
    "    \"GPT-4o\": 128_000,\n",
    "    \"Claude 3 Sonnet\": 200_000,\n",
    "    \"Claude 3 Opus\": 200_000,\n",
    "}\n",
    "\n",
    "# How much text fits?\n",
    "sample_text = \"\"\"This is a sample paragraph of text that represents typical \n",
    "content you might send to an LLM. It contains normal English sentences with \n",
    "common words and punctuation. We can use this to estimate how much content \n",
    "fits in different context windows.\"\"\"\n",
    "\n",
    "tokens_per_char = count_tokens(sample_text) / len(sample_text)\n",
    "\n",
    "print(\"Approximate capacity (leaving room for response):\\n\")\n",
    "for model, window in context_windows.items():\n",
    "    # Assume we want to leave 25% for the response\n",
    "    usable = int(window * 0.75)\n",
    "    approx_chars = int(usable / tokens_per_char)\n",
    "    approx_pages = approx_chars / 3000  # ~3000 chars per page\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  {usable:,} tokens for input (~{approx_pages:.0f} pages of text)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text tokens: 1,001\n",
      "Available: 96,000 (reserving 25% for response)\n",
      "Fits: Yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if your text fits in context\n",
    "def will_it_fit(text, model_context=128000, reserve_for_response=0.25):\n",
    "    \"\"\"Check if text fits in a model's context window.\"\"\"\n",
    "    tokens = count_tokens(text)\n",
    "    available = int(model_context * (1 - reserve_for_response))\n",
    "    fits = tokens <= available\n",
    "    \n",
    "    print(f\"Text tokens: {tokens:,}\")\n",
    "    print(f\"Available: {available:,} (reserving {reserve_for_response:.0%} for response)\")\n",
    "    print(f\"Fits: {'Yes' if fits else 'No'}\")\n",
    "    if not fits:\n",
    "        print(f\"Over by: {tokens - available:,} tokens\")\n",
    "    return fits\n",
    "\n",
    "# Test with a longer text\n",
    "long_text = \"The quick brown fox jumps over the lazy dog. \" * 100\n",
    "will_it_fit(long_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. **Tokens are not words** - They're chunks that can be whole words, word pieces, or characters\n",
    "2. **Case matters** - \"Hello\" and \"hello\" are different tokens\n",
    "3. **Common = compact** - Frequent words use fewer tokens than rare ones\n",
    "4. **Spaces attach** - \" word\" (with space) is often one token, different from \"word\"\n",
    "5. **Costs add up** - Being concise saves money at scale\n",
    "6. **Context has limits** - Know your model's window size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "Use this cell to experiment with your own text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Your text here'\n",
      "Total tokens: 3\n",
      "\n",
      "Breakdown:\n",
      "  1. ID   7927 -> 'Your'\n",
      "  2. ID   1495 -> ' text'\n",
      "  3. ID   1618 -> ' here'\n"
     ]
    }
   ],
   "source": [
    "# Try your own text here!\n",
    "my_text = \"Your text here\"\n",
    "\n",
    "show_tokens(my_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
